{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "random_input = torch.randn(size=(100,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "random_input = torch.randn(size=(100,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_tree=5,\n",
    "     tree_depth=3,\n",
    "     n_in_feature=5,\n",
    "     tree_feature_rate=0.1,\n",
    "     n_class=2,\n",
    "     jointly_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_feature = random_input.shape[1]\n",
    "depth = 3 \n",
    "n_leaf = 2 ** depth\n",
    "used_feature_rate = 0.4\n",
    "n_class = 2 \n",
    "n_used_feature = int(n_in_feature*used_feature_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params=dict(\n",
    "    depth=depth,\n",
    "    n_in_feature = random_input.shape[1],\n",
    "    used_feature_rate = 0.4,\n",
    "    n_class=2, \n",
    "    jointly_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.eye(n_in_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 14,  5,  2,  0, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using_idx = np.random.choice(np.arange(n_in_feature), n_used_feature, replace=False)\n",
    "using_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mask = onehot[using_idx].T\n",
    "from torch.nn.parameter import Parameter\n",
    "feature_mask = Parameter(torch.from_numpy(feature_mask).type(torch.FloatTensor),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.4753, 0.0416],\n",
       "        [0.7423, 0.3228],\n",
       "        [0.4248, 0.3834],\n",
       "        [0.5319, 0.6228],\n",
       "        [0.3969, 0.5138],\n",
       "        [0.9407, 0.6633],\n",
       "        [0.0394, 0.9293],\n",
       "        [0.2009, 0.4902]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.random.rand(n_leaf,n_class)\n",
    "pi = Parameter(torch.from_numpy(pi).type(torch.FloatTensor),requires_grad=True)\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (linear1): Linear(in_features=6, out_features=8, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "decision = nn.Sequential(OrderedDict([\n",
    "                        ('linear1',nn.Linear(n_used_feature,n_leaf)),\n",
    "                        ('sigmoid', nn.Sigmoid()),\n",
    "                        ]))\n",
    "decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 6]), torch.Size([100, 16]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mask.shape , random_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "class Tree(nn.Module) :\n",
    "    def __init__(self, depth, n_in_feature, used_feature_rate, n_class , jointly_training=True) :\n",
    "        self.depth = depth\n",
    "        self.n_leaf = 2 ** depth\n",
    "        self.n_class = n_class\n",
    "        self.jointly_training = jointly_training\n",
    "        # used features in this tree\n",
    "        n_used_feature = int(n_in_feature*used_feature_rate)\n",
    "        onehot = np.eye(n_in_feature)\n",
    "        using_idx = np.random.choice(np.arange(n_in_feature), n_used_feature, replace=False)\n",
    "        self.feature_mask = onehot[using_idx].T\n",
    "        self.feature_mask = Parameter(torch.from_numpy(self.feature_mask).type(torch.FloatTensor),requires_grad=False)\n",
    "        # leaf label distribution\n",
    "        if jointly_training:\n",
    "            self.pi = np.random.rand(self.n_leaf,n_class)\n",
    "            self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor),requires_grad=True)\n",
    "        else:\n",
    "            self.pi = np.ones((self.n_leaf, n_class)) / n_class\n",
    "            self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor), requires_grad=False)\n",
    "        \n",
    "        self.decision = nn.Sequential(OrderedDict([\n",
    "                        ('linear1',nn.Linear(n_used_feature,self.n_leaf)),\n",
    "                        ('sigmoid', nn.Sigmoid()),\n",
    "                        ]))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param x(Variable): [batch_size,n_features]\n",
    "        :return: route probability (Variable): [batch_size,n_leaf]\n",
    "        \"\"\"\n",
    "        if x.is_cuda and not self.feature_mask.is_cuda:\n",
    "            self.feature_mask = self.feature_mask.cuda()\n",
    "\n",
    "        feats = torch.mm(x,self.feature_mask) # ->[batch_size,n_used_feature]\n",
    "        decision = self.decision(feats) # ->[batch_size,n_leaf]\n",
    "\n",
    "        decision = torch.unsqueeze(decision,dim=2)\n",
    "        decision_comp = 1-decision\n",
    "        decision = torch.cat((decision,decision_comp),dim=2) # -> [batch_size,n_leaf,2]\n",
    "\n",
    "        # compute route probability\n",
    "        # note: we do not use decision[:,0]\n",
    "        batch_size = x.size()[0]\n",
    "        _mu = Variable(x.data.new(batch_size,1,1).fill_(1.))\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        for n_layer in range(0, self.depth):\n",
    "            _mu = _mu.view(batch_size,-1,1).repeat(1,1,2)\n",
    "            _decision = decision[:, begin_idx:end_idx, :]  # -> [batch_size,2**n_layer,2]\n",
    "            _mu = _mu*_decision # -> [batch_size,2**n_layer,2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (n_layer+1)\n",
    "\n",
    "        mu = _mu.view(batch_size,self.n_leaf)\n",
    "\n",
    "        return mu\n",
    "\n",
    "    def get_pi(self):\n",
    "        if self.jointly_training:\n",
    "            return F.softmax(self.pi,dim=-1)\n",
    "        else:\n",
    "            return self.pi\n",
    "\n",
    "    def cal_prob(self,mu,pi):\n",
    "        \"\"\"\n",
    "        :param mu [batch_size,n_leaf]\n",
    "        :param pi [n_leaf,n_class]\n",
    "        :return: label probability [batch_size,n_class]\n",
    "        \"\"\"\n",
    "        p = torch.mm(mu,pi)\n",
    "        return p\n",
    "\n",
    "\n",
    "    def update_pi(self,new_pi):\n",
    "        self.pi.data=new_pi\n",
    "\n",
    "import numpy as np\n",
    "tree_param = dict(depth = 3 ,\n",
    "                  n_in_feature = 16 ,\n",
    "                  used_feature_rate = 0.4 ,\n",
    "                  n_class = 2 , \n",
    "                  jointly_training=True )\n",
    "Tree(**tree_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(nn.Module):\n",
    "    def __init__(self,depth,n_in_feature,used_feature_rate,n_class, jointly_training=True,**kwargs):\n",
    "        super(Tree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.n_leaf = 2 ** depth\n",
    "        self.n_class = n_class\n",
    "        self.jointly_training = jointly_training\n",
    "\n",
    "        # used features in this tree\n",
    "        n_used_feature = int(n_in_feature*used_feature_rate)\n",
    "        onehot = np.eye(n_in_feature)\n",
    "        using_idx = np.random.choice(np.arange(n_in_feature), n_used_feature, replace=False)\n",
    "        self.feature_mask = onehot[using_idx].T\n",
    "        self.feature_mask = Parameter(torch.from_numpy(self.feature_mask).type(torch.FloatTensor),requires_grad=False)\n",
    "        # leaf label distribution\n",
    "        if jointly_training:\n",
    "            self.pi = np.random.rand(self.n_leaf,n_class)\n",
    "            self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor),requires_grad=True)\n",
    "        else:\n",
    "            self.pi = np.ones((self.n_leaf, n_class)) / n_class\n",
    "            self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor), requires_grad=False)\n",
    "\n",
    "        # decision\n",
    "        self.decision = nn.Sequential(OrderedDict([\n",
    "                        ('linear1',nn.Linear(n_used_feature,self.n_leaf)),\n",
    "                        ('sigmoid', nn.Sigmoid()),\n",
    "                        ]))\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param x(Variable): [batch_size,n_features]\n",
    "        :return: route probability (Variable): [batch_size,n_leaf]\n",
    "        \"\"\"\n",
    "        if x.is_cuda and not self.feature_mask.is_cuda:\n",
    "            self.feature_mask = self.feature_mask.cuda()\n",
    "        feats = torch.einsum(\"ij,jk->ik\",[x,self.feature_mask]) # ->[batch_size,n_used_feature]\n",
    "        decision = self.decision(feats) # ->[batch_size,n_leaf]\n",
    "\n",
    "        decision = torch.unsqueeze(decision,dim=2)\n",
    "        decision_comp = 1-decision\n",
    "        decision = torch.cat((decision,decision_comp),dim=2) # -> [batch_size,n_leaf,2]\n",
    "\n",
    "        # compute route probability\n",
    "        # note: we do not use decision[:,0]\n",
    "        batch_size = x.size()[0]\n",
    "        _mu = Variable(x.data.new(batch_size,1,1).fill_(1.))\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        for n_layer in range(0, self.depth):\n",
    "            _mu = _mu.view(batch_size,-1,1).repeat(1,1,2)\n",
    "            _decision = decision[:, begin_idx:end_idx, :]  # -> [batch_size,2**n_layer,2]\n",
    "            _mu = _mu*_decision # -> [batch_size,2**n_layer,2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (n_layer+1)\n",
    "\n",
    "        mu = _mu.view(batch_size,self.n_leaf)\n",
    "\n",
    "        return mu\n",
    "\n",
    "    def get_pi(self):\n",
    "        if self.jointly_training:\n",
    "            return F.softmax(self.pi,dim=-1)\n",
    "        else:\n",
    "            return self.pi\n",
    "\n",
    "    def cal_prob(self,mu,pi):\n",
    "        \"\"\"\n",
    "        :param mu [batch_size,n_leaf]\n",
    "        :param pi [n_leaf,n_class]\n",
    "        :return: label probability [batch_size,n_class]\n",
    "        \"\"\"\n",
    "        p=torch.einsum(\"ij,jk->ik\",[mu,pi])\n",
    "        return p\n",
    "\n",
    "\n",
    "    def update_pi(self,new_pi):\n",
    "        self.pi.data=new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_tree': 5,\n",
       " 'tree_depth': 3,\n",
       " 'n_in_feature': 5,\n",
       " 'tree_feature_rate': 0.1,\n",
       " 'n_class': 2,\n",
       " 'jointly_training': True}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = torch.einsum(\"ij,jk->ik\",[random_input, feature_mask])\n",
    "decision_ = decision(feats)\n",
    "decision_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_ = torch.unsqueeze(decision_,dim=2)\n",
    "decision_comp = 1-decision_\n",
    "decision__ = torch.cat((decision_,decision_comp),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 2])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision__.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "batch_size = random_input.size()[0]\n",
    "_mu = Variable(random_input.data.new(batch_size,1,1).fill_(1.))\n",
    "_mu.shape\n",
    "_mu.view(batch_size,-1,1).repeat(1,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision__.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "begin_idx = 1\n",
    "end_idx = 2\n",
    "for n_layer in range(0, depth):\n",
    "    _mu = _mu.view(batch_size,-1,1).repeat(1,1,2)\n",
    "    _decision = decision__[:, begin_idx:end_idx, :]  # -> [batch_size,2**n_layer,2]\n",
    "    _mu = _mu*_decision # -> [batch_size,2**n_layer,2]\n",
    "    begin_idx = end_idx\n",
    "    end_idx = begin_idx + 2 ** (n_layer+1)\n",
    "else :\n",
    "    print(_mu.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0605, 0.0966, 0.1787, 0.1497, 0.1074, 0.1621, 0.1098, 0.1351],\n",
       "        [0.0964, 0.1322, 0.1857, 0.1255, 0.1015, 0.1377, 0.1179, 0.1030],\n",
       "        [0.0651, 0.2369, 0.3058, 0.1876, 0.0146, 0.0653, 0.0774, 0.0473],\n",
       "        [0.1028, 0.1340, 0.2970, 0.1987, 0.0311, 0.0921, 0.1208, 0.0235],\n",
       "        [0.1664, 0.1851, 0.2532, 0.2544, 0.0073, 0.0595, 0.0495, 0.0244],\n",
       "        [0.0856, 0.0672, 0.0537, 0.0486, 0.3204, 0.1545, 0.0585, 0.2115],\n",
       "        [0.0615, 0.0280, 0.0986, 0.1576, 0.2292, 0.2623, 0.1078, 0.0550],\n",
       "        [0.1315, 0.1039, 0.1762, 0.2412, 0.0574, 0.1838, 0.0577, 0.0483],\n",
       "        [0.1121, 0.0781, 0.1231, 0.1610, 0.1093, 0.1917, 0.1227, 0.1018],\n",
       "        [0.0360, 0.0522, 0.2684, 0.1950, 0.1055, 0.1742, 0.1302, 0.0385],\n",
       "        [0.0532, 0.0617, 0.0780, 0.0685, 0.2224, 0.1394, 0.1287, 0.2479],\n",
       "        [0.1331, 0.1584, 0.1673, 0.1108, 0.0838, 0.1203, 0.1148, 0.1115],\n",
       "        [0.0803, 0.0579, 0.0542, 0.0746, 0.2691, 0.2063, 0.0588, 0.1989],\n",
       "        [0.1183, 0.1172, 0.1389, 0.1653, 0.1064, 0.2072, 0.0393, 0.1074],\n",
       "        [0.1026, 0.1132, 0.1463, 0.1147, 0.1254, 0.1507, 0.1109, 0.1362],\n",
       "        [0.0477, 0.0645, 0.2105, 0.2205, 0.0911, 0.1961, 0.1067, 0.0629],\n",
       "        [0.1441, 0.1474, 0.2037, 0.1219, 0.0956, 0.1339, 0.0783, 0.0751],\n",
       "        [0.2128, 0.0729, 0.0731, 0.1210, 0.1328, 0.2209, 0.0619, 0.1045],\n",
       "        [0.0959, 0.0677, 0.1036, 0.0668, 0.2768, 0.1343, 0.1857, 0.0693],\n",
       "        [0.0976, 0.1268, 0.2230, 0.1362, 0.1019, 0.1413, 0.0970, 0.0761],\n",
       "        [0.0674, 0.0474, 0.1393, 0.1462, 0.2208, 0.2244, 0.0849, 0.0697],\n",
       "        [0.1261, 0.1213, 0.1222, 0.0685, 0.2189, 0.1444, 0.0506, 0.1479],\n",
       "        [0.0649, 0.1328, 0.2105, 0.0943, 0.1094, 0.1012, 0.2039, 0.0830],\n",
       "        [0.0892, 0.1918, 0.3146, 0.1832, 0.0241, 0.0831, 0.0697, 0.0441],\n",
       "        [0.1188, 0.1737, 0.1516, 0.0900, 0.1083, 0.1218, 0.1216, 0.1143],\n",
       "        [0.1437, 0.0538, 0.0822, 0.1166, 0.2195, 0.2267, 0.1037, 0.0538],\n",
       "        [0.0966, 0.0967, 0.1898, 0.1579, 0.0952, 0.1504, 0.1549, 0.0585],\n",
       "        [0.0933, 0.1707, 0.2937, 0.1663, 0.0402, 0.1014, 0.0714, 0.0631],\n",
       "        [0.0955, 0.1409, 0.2137, 0.1065, 0.1084, 0.1197, 0.1174, 0.0980],\n",
       "        [0.0398, 0.0220, 0.0436, 0.0451, 0.4331, 0.1326, 0.1545, 0.1294],\n",
       "        [0.0722, 0.1036, 0.1332, 0.1125, 0.1242, 0.1436, 0.1773, 0.1335],\n",
       "        [0.0981, 0.1098, 0.1523, 0.1170, 0.1356, 0.1595, 0.0827, 0.1450],\n",
       "        [0.1283, 0.1077, 0.2104, 0.1944, 0.0743, 0.1613, 0.0865, 0.0371],\n",
       "        [0.1100, 0.1911, 0.2022, 0.1668, 0.0478, 0.1267, 0.0710, 0.0843],\n",
       "        [0.0507, 0.2284, 0.3428, 0.1488, 0.0212, 0.0650, 0.0822, 0.0608],\n",
       "        [0.1334, 0.1186, 0.2216, 0.2034, 0.0725, 0.1682, 0.0472, 0.0352],\n",
       "        [0.0792, 0.0655, 0.1098, 0.1269, 0.1811, 0.1999, 0.1476, 0.0899],\n",
       "        [0.0780, 0.0872, 0.0855, 0.0582, 0.2309, 0.1241, 0.1782, 0.1579],\n",
       "        [0.1324, 0.1519, 0.1610, 0.1514, 0.0798, 0.1592, 0.0780, 0.0863],\n",
       "        [0.1047, 0.0938, 0.1115, 0.1490, 0.1328, 0.2179, 0.0690, 0.1213],\n",
       "        [0.1046, 0.1284, 0.1492, 0.1319, 0.1116, 0.1684, 0.0677, 0.1382],\n",
       "        [0.0653, 0.1929, 0.2605, 0.1568, 0.0291, 0.0792, 0.0994, 0.1168],\n",
       "        [0.1114, 0.0861, 0.1171, 0.1081, 0.1932, 0.1886, 0.0767, 0.1188],\n",
       "        [0.0978, 0.0678, 0.2122, 0.1972, 0.1282, 0.2037, 0.0546, 0.0384],\n",
       "        [0.1737, 0.2199, 0.1466, 0.0731, 0.0629, 0.0840, 0.0923, 0.1475],\n",
       "        [0.1455, 0.0949, 0.1126, 0.1118, 0.1830, 0.2023, 0.0402, 0.1096],\n",
       "        [0.0723, 0.0901, 0.1555, 0.0956, 0.1455, 0.1223, 0.2044, 0.1143],\n",
       "        [0.0977, 0.3323, 0.2380, 0.1410, 0.0109, 0.0505, 0.0539, 0.0757],\n",
       "        [0.0939, 0.1792, 0.1871, 0.1037, 0.0753, 0.0994, 0.1879, 0.0736],\n",
       "        [0.1203, 0.0851, 0.1340, 0.1351, 0.1507, 0.1905, 0.1010, 0.0834],\n",
       "        [0.1212, 0.2070, 0.1477, 0.1301, 0.0525, 0.1205, 0.0740, 0.1470],\n",
       "        [0.1487, 0.2116, 0.1550, 0.1508, 0.0371, 0.1118, 0.0779, 0.1071],\n",
       "        [0.0520, 0.0419, 0.2035, 0.2083, 0.1364, 0.2193, 0.0958, 0.0428],\n",
       "        [0.0873, 0.1533, 0.1557, 0.0959, 0.1112, 0.1236, 0.1228, 0.1501],\n",
       "        [0.0649, 0.0344, 0.0993, 0.1572, 0.2101, 0.2617, 0.0846, 0.0878],\n",
       "        [0.0512, 0.0668, 0.1766, 0.1145, 0.1663, 0.1428, 0.2229, 0.0589],\n",
       "        [0.1426, 0.0774, 0.1537, 0.1206, 0.1742, 0.1712, 0.1071, 0.0532],\n",
       "        [0.0375, 0.0368, 0.1297, 0.1935, 0.1650, 0.2637, 0.1139, 0.0600],\n",
       "        [0.0484, 0.0746, 0.2225, 0.2252, 0.0928, 0.2083, 0.0505, 0.0778],\n",
       "        [0.1378, 0.1967, 0.2916, 0.1114, 0.0688, 0.1033, 0.0487, 0.0415],\n",
       "        [0.1190, 0.1081, 0.0949, 0.1661, 0.0735, 0.1907, 0.0755, 0.1722],\n",
       "        [0.0828, 0.0573, 0.1273, 0.1456, 0.1675, 0.2069, 0.1120, 0.1005],\n",
       "        [0.1353, 0.1159, 0.1053, 0.0856, 0.1679, 0.1563, 0.1188, 0.1149],\n",
       "        [0.1378, 0.0681, 0.0744, 0.0663, 0.2746, 0.1679, 0.0877, 0.1233],\n",
       "        [0.1798, 0.1167, 0.1555, 0.1315, 0.1144, 0.1709, 0.0441, 0.0871],\n",
       "        [0.0719, 0.0475, 0.1153, 0.1075, 0.2408, 0.1812, 0.1650, 0.0708],\n",
       "        [0.0767, 0.0561, 0.1040, 0.1079, 0.2386, 0.1958, 0.1349, 0.0860],\n",
       "        [0.0473, 0.0484, 0.0299, 0.0171, 0.3662, 0.0660, 0.1791, 0.2460],\n",
       "        [0.0634, 0.1011, 0.2271, 0.2954, 0.0329, 0.1554, 0.0478, 0.0770],\n",
       "        [0.0598, 0.0744, 0.2090, 0.3412, 0.0316, 0.1713, 0.0618, 0.0509],\n",
       "        [0.0369, 0.0515, 0.1250, 0.1439, 0.1861, 0.2218, 0.1137, 0.1210],\n",
       "        [0.1599, 0.1287, 0.1057, 0.0379, 0.2103, 0.0873, 0.1500, 0.1202],\n",
       "        [0.1158, 0.0817, 0.0804, 0.0553, 0.2605, 0.1349, 0.1166, 0.1549],\n",
       "        [0.1100, 0.1044, 0.1282, 0.0970, 0.1446, 0.1435, 0.1234, 0.1489],\n",
       "        [0.1313, 0.1518, 0.0720, 0.0657, 0.0918, 0.1098, 0.0982, 0.2793],\n",
       "        [0.1203, 0.1412, 0.1655, 0.1401, 0.0711, 0.1336, 0.0878, 0.1404],\n",
       "        [0.0714, 0.0589, 0.0976, 0.1322, 0.1935, 0.2284, 0.0966, 0.1214],\n",
       "        [0.0578, 0.0337, 0.0753, 0.1144, 0.2770, 0.2426, 0.0960, 0.1033],\n",
       "        [0.0847, 0.0421, 0.0702, 0.1696, 0.1640, 0.3050, 0.0397, 0.1246],\n",
       "        [0.1055, 0.1301, 0.2066, 0.2237, 0.0485, 0.1545, 0.0614, 0.0697],\n",
       "        [0.0685, 0.1846, 0.2568, 0.1393, 0.0476, 0.0926, 0.1586, 0.0520],\n",
       "        [0.0662, 0.0715, 0.0831, 0.0536, 0.2334, 0.1136, 0.1163, 0.2624],\n",
       "        [0.0899, 0.0947, 0.1387, 0.1584, 0.1416, 0.2171, 0.0685, 0.0910],\n",
       "        [0.0877, 0.1686, 0.1261, 0.0491, 0.1428, 0.0844, 0.1559, 0.1856],\n",
       "        [0.0294, 0.0519, 0.2428, 0.1345, 0.1837, 0.1680, 0.1450, 0.0447],\n",
       "        [0.1136, 0.1385, 0.1622, 0.1619, 0.0694, 0.1487, 0.1119, 0.0938],\n",
       "        [0.1744, 0.1056, 0.1819, 0.1635, 0.0692, 0.1413, 0.0964, 0.0678],\n",
       "        [0.1375, 0.2089, 0.1745, 0.2202, 0.0216, 0.1134, 0.0412, 0.0827],\n",
       "        [0.1045, 0.1738, 0.2008, 0.0781, 0.0792, 0.0776, 0.1716, 0.1143],\n",
       "        [0.1390, 0.1398, 0.2976, 0.2067, 0.0292, 0.0996, 0.0594, 0.0287],\n",
       "        [0.1330, 0.1401, 0.2600, 0.1956, 0.0465, 0.1255, 0.0675, 0.0320],\n",
       "        [0.0687, 0.1057, 0.1219, 0.0995, 0.1601, 0.1584, 0.1116, 0.1741],\n",
       "        [0.1494, 0.2213, 0.1734, 0.1427, 0.0496, 0.1290, 0.0384, 0.0963],\n",
       "        [0.0531, 0.0182, 0.0863, 0.1061, 0.3232, 0.2068, 0.1405, 0.0657],\n",
       "        [0.1498, 0.1232, 0.1879, 0.1371, 0.0911, 0.1424, 0.0883, 0.0802],\n",
       "        [0.0555, 0.0232, 0.0619, 0.1258, 0.2658, 0.2730, 0.1072, 0.0876],\n",
       "        [0.0778, 0.0304, 0.0610, 0.0900, 0.2807, 0.2067, 0.1330, 0.1203],\n",
       "        [0.0678, 0.0962, 0.1191, 0.0830, 0.1677, 0.1326, 0.1430, 0.1906],\n",
       "        [0.0865, 0.1302, 0.2503, 0.1938, 0.0549, 0.1370, 0.0717, 0.0756],\n",
       "        [0.1062, 0.0777, 0.1385, 0.1018, 0.1994, 0.1616, 0.1288, 0.0861]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = _mu.view(batch_size,n_leaf)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "pi_prob = F.softmax(pi,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 8]), torch.Size([8, 2]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.size() ,pi_prob.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = Tree(**tree_params)\n",
    "tree2 = Tree(**tree_params)\n",
    "mu = tree1(random_input)\n",
    "p1=tree1.cal_prob(mu,tree1.get_pi())\n",
    "mu = tree2(random_input)\n",
    "p2=tree2.cal_prob(mu,tree2.get_pi())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum() the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39689/2174882826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ijk,ikj->ij\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum() the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "torch.einsum(\"ijk,ikj->ij\",[p1,p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5345, 0.4655],\n",
       "        [0.5357, 0.4643],\n",
       "        [0.5222, 0.4778],\n",
       "        [0.5030, 0.4970],\n",
       "        [0.5286, 0.4714],\n",
       "        [0.5199, 0.4801],\n",
       "        [0.5164, 0.4836],\n",
       "        [0.5307, 0.4693],\n",
       "        [0.5256, 0.4744],\n",
       "        [0.5149, 0.4851],\n",
       "        [0.5164, 0.4836],\n",
       "        [0.5329, 0.4671],\n",
       "        [0.5195, 0.4805],\n",
       "        [0.5173, 0.4827],\n",
       "        [0.5278, 0.4722],\n",
       "        [0.5162, 0.4838],\n",
       "        [0.5209, 0.4791],\n",
       "        [0.5380, 0.4620],\n",
       "        [0.5176, 0.4824],\n",
       "        [0.5242, 0.4758],\n",
       "        [0.5251, 0.4749],\n",
       "        [0.5326, 0.4674],\n",
       "        [0.5315, 0.4685],\n",
       "        [0.5157, 0.4843],\n",
       "        [0.5245, 0.4755],\n",
       "        [0.5232, 0.4768],\n",
       "        [0.5146, 0.4854],\n",
       "        [0.5280, 0.4720],\n",
       "        [0.5199, 0.4801],\n",
       "        [0.5237, 0.4763],\n",
       "        [0.5133, 0.4867],\n",
       "        [0.5258, 0.4742],\n",
       "        [0.5354, 0.4646],\n",
       "        [0.5128, 0.4872],\n",
       "        [0.5269, 0.4731],\n",
       "        [0.5246, 0.4754],\n",
       "        [0.5340, 0.4660],\n",
       "        [0.5190, 0.4810],\n",
       "        [0.5331, 0.4669],\n",
       "        [0.5174, 0.4826],\n",
       "        [0.5307, 0.4693],\n",
       "        [0.5240, 0.4760],\n",
       "        [0.5194, 0.4806],\n",
       "        [0.5315, 0.4685],\n",
       "        [0.5369, 0.4631],\n",
       "        [0.5296, 0.4704],\n",
       "        [0.5179, 0.4821],\n",
       "        [0.5108, 0.4892],\n",
       "        [0.5284, 0.4716],\n",
       "        [0.5097, 0.4903],\n",
       "        [0.5318, 0.4682],\n",
       "        [0.5234, 0.4766],\n",
       "        [0.5261, 0.4739],\n",
       "        [0.5232, 0.4768],\n",
       "        [0.5222, 0.4778],\n",
       "        [0.5170, 0.4830],\n",
       "        [0.5333, 0.4667],\n",
       "        [0.5159, 0.4841],\n",
       "        [0.5272, 0.4728],\n",
       "        [0.5350, 0.4650],\n",
       "        [0.5177, 0.4823],\n",
       "        [0.5271, 0.4729],\n",
       "        [0.5265, 0.4735],\n",
       "        [0.5272, 0.4728],\n",
       "        [0.5237, 0.4763],\n",
       "        [0.5315, 0.4685],\n",
       "        [0.5203, 0.4797],\n",
       "        [0.5305, 0.4695],\n",
       "        [0.5193, 0.4807],\n",
       "        [0.5128, 0.4872],\n",
       "        [0.5163, 0.4837],\n",
       "        [0.5304, 0.4696],\n",
       "        [0.5348, 0.4652],\n",
       "        [0.5341, 0.4659],\n",
       "        [0.5179, 0.4821],\n",
       "        [0.5250, 0.4750],\n",
       "        [0.5105, 0.4895],\n",
       "        [0.5264, 0.4736],\n",
       "        [0.5174, 0.4826],\n",
       "        [0.5196, 0.4804],\n",
       "        [0.5151, 0.4849],\n",
       "        [0.5302, 0.4698],\n",
       "        [0.5268, 0.4732],\n",
       "        [0.5380, 0.4620],\n",
       "        [0.5276, 0.4724],\n",
       "        [0.5158, 0.4842],\n",
       "        [0.5359, 0.4641],\n",
       "        [0.5151, 0.4849],\n",
       "        [0.5127, 0.4873],\n",
       "        [0.5232, 0.4768],\n",
       "        [0.5286, 0.4714],\n",
       "        [0.5276, 0.4724],\n",
       "        [0.5229, 0.4771],\n",
       "        [0.5239, 0.4761],\n",
       "        [0.5247, 0.4753],\n",
       "        [0.5171, 0.4829],\n",
       "        [0.5250, 0.4750],\n",
       "        [0.5247, 0.4753],\n",
       "        [0.5158, 0.4842],\n",
       "        [0.5337, 0.4663]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cat([p1.unsqueeze(2),p2.unsqueeze(2)],dim=2).size())\n",
    "torch.sum(torch.cat([p1.unsqueeze(2),p2.unsqueeze(2)],dim=2),dim=2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "random_input = torch.randn(size=(100,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4903, 0.5097],\n",
       "        [0.4975, 0.5025],\n",
       "        [0.5148, 0.4852],\n",
       "        [0.5019, 0.4981],\n",
       "        [0.5255, 0.4745],\n",
       "        [0.4894, 0.5106],\n",
       "        [0.4918, 0.5082],\n",
       "        [0.5166, 0.4834],\n",
       "        [0.4947, 0.5053],\n",
       "        [0.4866, 0.5134],\n",
       "        [0.4697, 0.5303],\n",
       "        [0.5037, 0.4963],\n",
       "        [0.4932, 0.5068],\n",
       "        [0.5177, 0.4823],\n",
       "        [0.4954, 0.5046],\n",
       "        [0.4930, 0.5070],\n",
       "        [0.5147, 0.4853],\n",
       "        [0.5192, 0.4808],\n",
       "        [0.4742, 0.5258],\n",
       "        [0.5038, 0.4962],\n",
       "        [0.4965, 0.5035],\n",
       "        [0.5081, 0.4919],\n",
       "        [0.4759, 0.5241],\n",
       "        [0.5158, 0.4842],\n",
       "        [0.5018, 0.4982],\n",
       "        [0.5028, 0.4972],\n",
       "        [0.4897, 0.5103],\n",
       "        [0.5132, 0.4868],\n",
       "        [0.4981, 0.5019],\n",
       "        [0.4609, 0.5391],\n",
       "        [0.4769, 0.5231],\n",
       "        [0.5001, 0.4999],\n",
       "        [0.5108, 0.4892],\n",
       "        [0.5163, 0.4837],\n",
       "        [0.5115, 0.4885],\n",
       "        [0.5213, 0.4787],\n",
       "        [0.4847, 0.5153],\n",
       "        [0.4702, 0.5298],\n",
       "        [0.5142, 0.4858],\n",
       "        [0.5067, 0.4933],\n",
       "        [0.5073, 0.4927],\n",
       "        [0.5015, 0.4985],\n",
       "        [0.5024, 0.4976],\n",
       "        [0.5113, 0.4887],\n",
       "        [0.5152, 0.4848],\n",
       "        [0.5163, 0.4837],\n",
       "        [0.4697, 0.5303],\n",
       "        [0.5305, 0.4695],\n",
       "        [0.4882, 0.5118],\n",
       "        [0.5016, 0.4984],\n",
       "        [0.5137, 0.4863],\n",
       "        [0.5186, 0.4814],\n",
       "        [0.4953, 0.5047],\n",
       "        [0.4935, 0.5065],\n",
       "        [0.4959, 0.5041],\n",
       "        [0.4658, 0.5342],\n",
       "        [0.5026, 0.4974],\n",
       "        [0.4900, 0.5100],\n",
       "        [0.5056, 0.4944],\n",
       "        [0.5275, 0.4725],\n",
       "        [0.5040, 0.4960],\n",
       "        [0.4915, 0.5085],\n",
       "        [0.4984, 0.5016],\n",
       "        [0.4974, 0.5026],\n",
       "        [0.5228, 0.4772],\n",
       "        [0.4772, 0.5228],\n",
       "        [0.4848, 0.5152],\n",
       "        [0.4487, 0.5513],\n",
       "        [0.5071, 0.4929],\n",
       "        [0.5028, 0.4972],\n",
       "        [0.4847, 0.5153],\n",
       "        [0.4905, 0.5095],\n",
       "        [0.4866, 0.5134],\n",
       "        [0.4909, 0.5091],\n",
       "        [0.4933, 0.5067],\n",
       "        [0.5047, 0.4953],\n",
       "        [0.4929, 0.5071],\n",
       "        [0.4890, 0.5110],\n",
       "        [0.5093, 0.4907],\n",
       "        [0.5132, 0.4868],\n",
       "        [0.4940, 0.5060],\n",
       "        [0.4720, 0.5280],\n",
       "        [0.5072, 0.4928],\n",
       "        [0.4827, 0.5173],\n",
       "        [0.4807, 0.5193],\n",
       "        [0.5026, 0.4974],\n",
       "        [0.5104, 0.4896],\n",
       "        [0.5257, 0.4743],\n",
       "        [0.4884, 0.5116],\n",
       "        [0.5192, 0.4808],\n",
       "        [0.5178, 0.4822],\n",
       "        [0.4877, 0.5123],\n",
       "        [0.5299, 0.4701],\n",
       "        [0.4768, 0.5232],\n",
       "        [0.5105, 0.4895],\n",
       "        [0.4885, 0.5115],\n",
       "        [0.4797, 0.5203],\n",
       "        [0.4772, 0.5228],\n",
       "        [0.5083, 0.4917],\n",
       "        [0.4907, 0.5093]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.einsum(\"ij,jk->ik\",[mu,pi_prob])\n",
    "torch.einsum(\"ij,ik->ik\",[p,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73615673a24b196790e917c7839e5f6021828cdacc9266d5cb2ad80ddd7580b0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('xai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
